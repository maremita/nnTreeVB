## ############################################################
## Template config file for write_run_nntreevb_exps.py
## 
## Hyper-parameters and default values 
## ############################################################

## ############################################################
## write_run_nntreevb_exps.py configuration sections
## ############################################################

## [slurm] and [evaluation] sections will be removed in the 
## final config files for nntreevb.py

[slurm]
run_slurm = False
## if run_slurm=False: the script will launch the jobs locally
## elif run_slurm=True: it will launch the jobs on slurm

## ############################################################
## WARNING:
## In the case of run_slurm=False, the script will run
## several tasks locally in the background. Make sure that you
## have the necessary resources on your machine.
## It can crash the system.
## You can modify the grid of parameters to be evaluated here 
## to reduce the number of scenarios.
## ############################################################

## Account: UPDATE your slurm account information here
account = ctb-banire
#account = def-banire
mail_user = amine.m.remita@gmail.com

## SLURM parameters
## ################
# If gpus_per_node > 0, device in [settings] will be updated 
# to cuda
gpus_per_node = 0
cpus_per_task = 12
exec_time = 15:00:00
mem = 80000M
## For testing
#exec_time = 00:05:00
#mem = 8000M

[evaluation]
#run_jobs = False
run_jobs = True
## If run_jobs = False: the script generates the nntreevb
## config files but it won't launch the jobs.
## If run_jobs=True: it runs the jobs

##
output_eval = ../exps/

#max_iter = 5000
#nb_replicates = 10
## For testing
max_iter = 500
nb_replicates = 2

## Remark: values can span multiple lines, as long as they are
## indented deeper than the first line of the value.
## Configparser fetchs the value as string, after that it will
## be casted to python objects using json.loads

#evaluations = {
#    "subs_model@sim_data": ["jc69", "gtr"],
#    "nb_sites@sim_data": ["100", "1000"],
#    "nb_taxa@sim_data": ["8", "16"],
#    "sim_blengths@sim_data": ["uniform(0.05, 0.2)", "exponential(10)"],
#    "subs_model@hyperparams": ["jc69", "gtr"],
#    "b_prior@hyperparams": ["exponential|10.|False", "gamma|0.1, 0.1|False"]
#    "b_var@hyperparams" : ["normal|0.1,0.1|lower_0", "gamma|0.1,1.0|False"]
#    }

evaluations = {
    "subs_model@sim_data": ["jc69", "gtr"],
    "nb_sites@sim_data": ["100", "1000"],
    "nb_taxa@sim_data": ["8", "16"]
    }

# [REQUIRED] Code names for evaluations
# It will be used to name jobs and output files
# It should have the same structure and item orders as
# evaluations option
eval_codes = {
    "d": ["jc69", "gtr"],
    "l": ["100", "1000"],
    "t": ["8", "16"]
    }

# #############################################################
# nnTreeVB template configuration sections
# #############################################################

[io]
# output_path is used to save training data, scores and figures
# It will be updated to ../exp_outputs/<jobs_code>
# <jobs_code> is defined in write_run_rep_exps.py
output_path = to_be_updated_by_the_script

# To use your FASTA files (real data for example), put sim_data
# to False and specify the path of FASTA files
seq_file = to_be_updated_by_the_script
nwk_file = to_be_updated_by_the_script

# If False the program runs the evaluation and save resutls in
# output_path,else the program loads the results directly from
# the file (if it exists)
scores_from_file = True

[sim_data]
# If sim_data = True: simulate a tree and evolve new sequences
# else: fetch sequences from seq_file and nwk_file
sim_data = True

# If seq_from_files and sim_data are True, the data will be
# extracted from simulated FASTA files if they already exist,
# else: new alignment will be simulated
seq_from_file = False
# If nwk_from_file and sim_data are True, the tree and 
# branch lengths will be used to simulate the alignment
# (sim_blengths will not be used in this case)
nwk_from_file = False

# nb_sites is the size of simulated alignment
# The site patterns are not unique
nb_sites = 1000
# nb_taxa is the number of species (leaves of the tree)
nb_taxa = 8

# Substitution model to be used for sequence simulation
# jc69, k80, hky, gtr 
subs_model = gtr

## Prior parameters for simulation
## ###############################
## IF sim_data is True, nntreevb.py will use the following 
## prior hyperparameters to simulate sequence alignments
## ELSE nntreevb.py will use them for comparison with
## its estimated parameters

## These priors are not used for inference. For inference
## prior check hyperparameters section

## Accepted distributions: (see torch_distributions.py module)
## normal, lognormal, gamma, dirichlet, categorical,
## exponential, uniform

# Branch lengths
# dist(param list)
# dist_ext(param list);dist_int(param list)
# Examples:
## Same prior for all all branches
sim_blengths = exponential(10.)
## Two priors for external and internal branches resp.
#sim_blengths = uniform(0.01, 0.05);uniform(0.01, 0.02)

# Substitution rates (GTR)
#sim_rates = dirichlet(1)
sim_rates = dirichlet(1., 1., 1., 1., 1., 1.)
#            AG     AC     AT     GC     GT     CT
#sim_rates = 0.160, 0.160, 0.160, 0.160, 0.160, 0.160
#sim_rates = 0.225, 0.106, 0.164, 0.064, 0.070, 0.369
#sim_rates = 0.160

# Relative frequencies (HKY and GTR)
#sim_freqss = dirichlet(1.)
sim_freqs = dirichlet(1., 1., 1., 1.)
#            A      G      C      T
#sim_freqs = 0.288, 0.190, 0.200, 0.326
# sim_freqs = 0.250

# Kappa (K80 and HKY)
sim_kappa = gamma(1., 1.)
#sim_kappa = 1.

# real_params is a flag to consider the parameters given
# in this section real or not. If they are, they will be used
# to compare with the estimated parameters by the variational
# model
real_params = False

[hyperparams]
# Hyper-parameters of the model set by user

# Substitution model to be used for model inference
# jc69 := infer b latent variables
# k80  := infer b, k latent variables
# hky  := infer b, k, f latent variables
# gtr  := infer b, r, f latent variables
subs_model = gtr

# Branches (All subs models)
#         distr. name|params of distr.|learn_prior[|lr]
b_prior = exponential|10.|False|0.1
#b_prior = gamma|0.1, 1.|True

#       distr. name|params|transform[|lr]
b_var = normal|0.1,0.1|lower_0|0.01
#b_var = gamma|0.1,1.0|False

# Tree length (Compound Gamma Dirichlet)
# Tree length will be estimated if branch lengths
# variational distribution is set to dirichlet
t_prior = gamma|1.,1.|False
t_var = normal|0.1,0.1|lower_0

# Rates (GTR)
#         distr. name| params| learn_prior
# the params of dirichlet are initialized using a uniform dist
r_prior = dirichlet|1.|False
r_var = normal|0.1,0.1|simplex

# Frequencies (HKY and GTR)
f_prior = dirichlet|uniform|False
f_var = normal|0.1,0.1|simplex

# Kappa (K80 and HKY)
k_prior = gamma|1.,1.|False
k_var = normal|0.1,0.1|lower_0

# Hyper parameters for (used) neural networks
h_dim = 16
nb_layers = 3
bias_layers = True
activ_layers = relu
dropout_layers = 0.

# fitting hyperparams
nb_replicates = 2
elbo_type = elbo
grad_samples = 1
K_grad_samples = 1
nb_samples = 10000
alpha_kl = 1.
max_iter = 5000
# optimizer type : adam | sgd
optimizer=adam
learning_rate = 0.1
weight_decay = 0.
save_fit_history = True
#save_val_history = False
save_grad_stats = True
save_weight_stats = True

[settings]
# job_name will be updated automatically
job_name = to_be_updated_by_the_script
# number of parallel replicates
n_parallel = ${hyperparams:nb_replicates}
# cpu | cuda | cuda:0
device = cpu
# Valid values for verbose: True, False, None and
# positive integers 
verbose = 1
compress_files = False

[plotting]
# To render Tex text in plots, Matplotlib requires
# Latex, dvipng, Ghostscript and type1ec.sty found in cm-super
# If Latex and other required packages are not installed,
# put to False
plt_usetex = False
size_font = 16
print_xtick_every = 100
logl_data=True
