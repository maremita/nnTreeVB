# #############################################################
# nnTreeVB template configuration file
# #############################################################

[io]
# output_path is used to save training data, scores and figures
#output_path = ../results/DS1
output_path = ../results/SIM

# To use your FASTA files (real data for example), put sim_data
# to False and specify the path of FASTA files
seq_file = ../example_data/DS/DS1/DS1.fasta
nwk_file = ../example_data/DS/DS1/DS1.nwk

# If False the program runs the evaluation and save resutls in
# output_path,else the program loads the results directly from the
# file (if it exists)
scores_from_file = True

# Name of the job. If 'auto' or 'none', and -j option has'nt 
# been set in the command line, the program generates
# automatically a name from timestamp
job_name = auto
#job_name = TEST

[data]
# If sim_data = True: simulate a tree and evolve new sequences
# else: fetch sequences from seq_file and nwk_file
sim_data = True

# nunmber of data replicates
nb_rep_data = 2

# If seq_from_files and sim_data are True, the data will be
# extracted from simulated FASTA files if they already exist,
# else: new alignment will be simulated
seq_from_file = False
# If nwk_from_file and sim_data are True, the tree and 
# branch lengths will be used to simulate the alignment
# (sim_blengths will not be used in this case)
nwk_from_file = False

# nb_sites is the size of simulated alignment
# The site patterns are not unique
nb_sites = 1000
# nb_taxa is the number of species (leaves of the tree)
nb_taxa = 8

# Substitution model to be used for sequence simulation
# jc69, k80, hky, gtr 
subs_model = jc69

## Prior parameters for simulation
## ###############################
## IF sim_data is True, nntreevb.py will use the following 
## prior hyperparameters to simulate sequence alignments
## ELSE nntreevb.py will use them for comparison with
## its estimated parameters

## These priors are not used for inference. For inference
## prior check hyperparameters section

## Accepted distributions: (see torch_distributions.py module)
## normal, lognormal, gamma, dirichlet, categorical,
## exponential, uniform

# Branch lengths
# dist(param list)
# dist_ext(param list);dist_int(param list)
# Examples:
## Same prior for all all branches
sim_blengths = exponential(10.)|True
#sim_blengths = exponential(10.);uniform(0.01, 0.1)|True
## Two priors for external and internal branches resp.
#sim_blengths = uniform(0.01, 0.05);uniform(0.01, 0.02)

# Substitution rates (GTR)
#sim_rates = dirichlet(1)
#sim_rates = dirichlet(1, 1, 1, 1, 1, 1)
sim_rates = dirichlet(1, 1, 1, 1, 1, 1)|True
#            AG     AC     AT     GC     GT     CT
#sim_rates = 0.160, 0.160, 0.160, 0.160, 0.160, 0.160
#sim_rates = 0.225, 0.106, 0.164, 0.064, 0.070, 0.369
#sim_rates = 0.160

# Relative frequencies (HKY and GTR)
#sim_freqss = dirichlet(1)
sim_freqs = dirichlet(1, 1, 1, 1)|True
#            A      G      C      T
#sim_freqs = 0.288, 0.190, 0.200, 0.326
# sim_freqs = 0.250

# Kappa (K80 and HKY)
sim_kappa = gamma(1., 1.)|True
#sim_kappa = 1.

# real_params is a flag to consider the parameters given
# in this section real or not. If they are, they will be used
# to compare with the estimated parameters by the variational
# model
real_params = True 

[hyperparams]
# Hyper-parameters of the model set by user

# Substitution model to be used for model inference
# jc69 := infer b latent variables
# k80  := infer b, k latent variables
# hky  := infer b, k, f latent variables
# gtr  := infer b, r, f latent variables
subs_model = gtr

# Branches (All subs models)
#         distr. name|params of distr.|learn_prior[|lr]
b_prior = exponential|10.|False
#b_prior = gamma|0.1, 1.|True

#       distr. name|params|transform[|lr]
b_var = normal|0.1,0.1|lower_0|0.1
#b_var = gamma|0.1,1.0|False

# Tree length (Compound Gamma Dirichlet)
# Tree length will be estimated if branch lengths
# variational distribution is set to dirichlet
t_prior = gamma|1.,1.|False
t_var = normal|0.1,0.1|lower_0

# Rates (GTR)
#         distr. name| params| learn_prior
# the params of dirichlet are initialized using a uniform dist
r_prior = dirichlet|1.|False
r_var = normal|0.1,0.1|simplex

# Frequencies (HKY and GTR)
f_prior = dirichlet|uniform|False
f_var = normal|0.1,0.1|simplex

# Kappa (K80 and HKY)
k_prior = gamma|1.,1.|False
k_var = normal|0.1,0.1|lower_0

# Hyper parameters for (used) neural networks
h_dim = 16
nb_layers = 3
bias_layers = True
activ_layers = relu
dropout_layers = 0.

# fitting hyperparams
nb_rep_fit = 3
elbo_type = elbo
grad_samples = 1
K_grad_samples = 1
# A large nb of samples can consume a lot of memory
nb_samples = 1000
alpha_kl = 1.
max_iter = 1000
# optimizer type : adam | sgd
optimizer=adam
learning_rate = 0.1
weight_decay = 0.
save_fit_history = True
#save_val_history = False
save_grad_stats = True
save_weight_stats = True

[settings]
# number of parallel replicates
n_parallel = 4
# cpu | cuda | cuda:0 | mps
device = cpu
# Valid values for verbose: True, False, None and
# positive integers 
verbose = 1
compress_files = True

[plotting]
# To render Tex text in plots, Matplotlib requires
# Latex, dvipng, Ghostscript and type1ec.sty found in cm-super
# If Latex and other required packages are not installed,
# put to False
plt_usetex = False
size_font = 16
print_xtick_every = 100
logl_data=True
